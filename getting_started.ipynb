{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ADIA Refactored â€“ Getting Started\n",
        "\n",
        "This notebook gives a gentle intro to working with the `adia_refactored` package from a notebook environment. It covers:\n",
        "\n",
        "- Ensuring your environment can import the package\n",
        "- A quick single-series demo\n",
        "- A small batch demo using Parquet files\n",
        "- (Optional) Running the included example script\n",
        "\n",
        "If you see import errors for `numpy`/`pandas`/`pyarrow`, run the optional install cell below and then restart the kernel.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install project dependencies into the active kernel\n",
        "# If imports fail later, run this, then restart the kernel and re-run\n",
        "import sys\n",
        "import subprocess\n",
        "\n",
        "def ensure_deps():\n",
        "    try:\n",
        "        import numpy  # noqa: F401\n",
        "        import pandas  # noqa: F401\n",
        "        import pyarrow  # noqa: F401\n",
        "        print(\"Dependencies appear to be installed. Skipping install.\")\n",
        "        return\n",
        "    except Exception:\n",
        "        pass\n",
        "    print(\"Installing requirements...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\"])  # nosec\n",
        "\n",
        "ensure_deps()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: make package importable and validate config\n",
        "import sys, pathlib\n",
        "\n",
        "project_root = pathlib.Path().resolve()\n",
        "parent_dir = project_root.parent\n",
        "if str(parent_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(parent_dir))\n",
        "\n",
        "from adia_refactored import compute_predictors_for_values, run_batch, validate_config\n",
        "validate_config()\n",
        "print(\"Imports OK and config validated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Single-series demo\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "n = 120\n",
        "bp = 60\n",
        "values = np.concatenate([\n",
        "    np.random.normal(0, 1.0, bp),\n",
        "    np.random.normal(1.5, 1.5, n - bp),\n",
        "])\n",
        "periods = np.concatenate([np.zeros(bp), np.ones(n - bp)])\n",
        "\n",
        "preds, meta = compute_predictors_for_values(values, periods, B_boot=20, energy_enable=False)\n",
        "preds, meta\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Batch demo (writes and reads Parquet files under _tmp_notebook)\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Prepare synthetic batch\n",
        "tmp_dir = Path(\"_tmp_notebook\"); tmp_dir.mkdir(exist_ok=True)\n",
        "frames = []\n",
        "for i in range(4):\n",
        "    np.random.seed(100 + i)\n",
        "    n = 80; bp = 40\n",
        "    if i % 2 == 0:\n",
        "        vals = np.concatenate([np.random.normal(0, 1.0, bp),\n",
        "                               np.random.normal(1.2, 1.0, n - bp)])\n",
        "    else:\n",
        "        vals = np.concatenate([np.random.normal(0, 0.8, bp),\n",
        "                               np.random.normal(0, 1.8, n - bp)])\n",
        "    periods = np.concatenate([np.zeros(bp), np.ones(n - bp)])\n",
        "    df = pd.DataFrame({\"value\": vals, \"period\": periods})\n",
        "    df.index = pd.MultiIndex.from_tuples([(f\"series_{i}\", t) for t in range(n)], names=[\"id\", \"time\"])\n",
        "    frames.append(df)\n",
        "\n",
        "all_df = pd.concat(frames).sort_index()\n",
        "\n",
        "in_path = tmp_dir / \"sample_batch.parquet\"\n",
        "out_pred = tmp_dir / \"sample_predictors.parquet\"\n",
        "out_meta = tmp_dir / \"sample_metadata.parquet\"\n",
        "\n",
        "all_df.to_parquet(in_path)\n",
        "\n",
        "pred_df, meta_df = run_batch(\n",
        "    input_parquet=str(in_path),\n",
        "    out_pred_parquet=str(out_pred),\n",
        "    out_meta_parquet=str(out_meta),\n",
        "    B_boot=20,\n",
        "    energy_enable=False,\n",
        "    n_jobs=1,\n",
        "    verbose=False,\n",
        ")\n",
        "\n",
        "print(f\"Processed series: {len(pred_df)}\")\n",
        "print(f\"Predictor columns: {len(pred_df.columns)}\")\n",
        "pred_df.head(), meta_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: run the included script and (optionally) clean up temp files\n",
        "print(\"Running example_usage.py ...\")\n",
        "%run example_usage.py\n",
        "\n",
        "# Optional cleanup for this notebook's temp files\n",
        "from pathlib import Path\n",
        "for p in Path(\"_tmp_notebook\").glob(\"*.parquet\"):\n",
        "    try:\n",
        "        p.unlink()\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
